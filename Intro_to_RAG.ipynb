{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/Generative-AI/blob/main/Intro_to_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain_core langchain_openai chroma sentence-transformers docling langchain-text-splitters \"langchain-chroma>=0.1.2\" langchain_huggingface https://gradio-builds.s3.amazonaws.com/a0c487cd57a217775f0d1bc77c041b7cd516cc8a/gradio-3.41.2-py3-none-any.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hgrPfh4comAx",
        "outputId": "44bff30c-9bff-4080-c043-cafd2c6fb971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Retrieval Augmented Generation (RAG)\n",
        "\n",
        "__Authored by__: Enrique Noriega-Atala\n",
        "__Last edited__: 10/07/2024\n",
        "\n",
        "Retrieval Augmented Generation is a method to prompt the LLM to elicit responses based on information retrieved from a data base, hence _retrieval_.\n",
        "\n",
        "RAG is a useful method to reduce LLM hallucinations and to _interface_ with documents in a more natural way, akin to a conversation rather than a search engine."
      ],
      "metadata": {
        "id": "WM9Lej7HoX3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Document Parsing"
      ],
      "metadata": {
        "id": "5QXgnvl-yukq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY5-ob1EoVX1"
      },
      "outputs": [],
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "source = \"https://arxiv.org/pdf/1706.03762\"  # PDF path or URL\n",
        "converter = DocumentConverter()\n",
        "result = converter.convert_single(source)\n",
        "markdown = result.render_as_markdown()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, Latex\n",
        "\n",
        "display(Markdown(markdown))"
      ],
      "metadata": {
        "id": "RJr6A2Cx4pah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Split the document"
      ],
      "metadata": {
        "id": "W7OlmFqq7blj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "]\n",
        "\n",
        "# MD splits\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
        ")\n",
        "md_header_splits = markdown_splitter.split_text(markdown)\n",
        "\n",
        "# Char-level splits\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunk_size = 250\n",
        "chunk_overlap = 30\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split\n",
        "splits = text_splitter.split_documents(md_header_splits)\n",
        "splits"
      ],
      "metadata": {
        "id": "BS2X5pCN6JWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total number of \"chunks\": {len(splits)}')"
      ],
      "metadata": {
        "id": "7IBrnqh27tvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Encode the splits into a _Vector Database_\n",
        "\n",
        "In order to execute _semantic_ queries, we need to generate _vector representations_ of each split \"chunk\".\n",
        "\n",
        "We will do this using `sentence-transformers` and `ChromaDB`"
      ],
      "metadata": {
        "id": "0Dastldi77jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "vector_store = Chroma(\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "# Index the documents using the embeddings model\n",
        "vector_store.add_documents(splits)"
      ],
      "metadata": {
        "id": "vs-SjKma711o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can do retrieval using sentence similarity\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "retriever.invoke(\"Attention is\")"
      ],
      "metadata": {
        "id": "iN8ukbbvQqB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can configure the parameters of the search, such as the number of chunks returned\n",
        "retriever = vector_store.as_retriever(search_kwargs={'k':10})\n",
        "retriever.invoke(\"Attention is\")"
      ],
      "metadata": {
        "id": "Yd_TqeGURAcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Connect the retrieval to an LLM\n",
        "\n",
        "We will use Verde for this purpose"
      ],
      "metadata": {
        "id": "SBSJPsKHRjTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "AmjpkeO9YQql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "API_ENDPOINT = \"https://llm1.cyverse.ai/v1\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"Mistral-7B-Instruct-v0.3\", base_url=API_ENDPOINT)\n",
        "\n",
        "llm.invoke(\"Hello! What can you tell me about yourself?\")"
      ],
      "metadata": {
        "id": "PNuwjojrRM4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have an llm client, we can wire togethe the retrieval component and the llm. First, let's try chaining the retriever to a prompt template, to get context aware prompts"
      ],
      "metadata": {
        "id": "K2JhdkRxZknC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pprint import pprint\n",
        "\n",
        "conversation_template = ChatPromptTemplate([\n",
        "    (\"system\", \"\"\"You will look at the following passages and reply to the question using only and only the information present in the following documents.\n",
        "         If the information requested is not present in the documents or if the question can not be answered using the documents as context, just say that you can't answer the question\n",
        "         ```{context}```\n",
        "         \"\"\"),\n",
        "    (\"human\", \"\"\"Question: ```{question}```\n",
        "    Answer: \"\"\")\n",
        "])\n",
        "\n",
        "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | conversation_template\n",
        "\n",
        "for message in chain.invoke(\"What is all I need?\").messages:\n",
        "  pprint((str(type(message)), message.content))"
      ],
      "metadata": {
        "id": "EvhRqWREYGt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe how the documents have a lot of irrelevant metadata.\n",
        "We need to format them better using a helper function"
      ],
      "metadata": {
        "id": "BUOkraKPcGQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def format_message(msgs):\n",
        "  return '\\n\\n'.join([f\"Section: {next(iter(msg.metadata.values()))}\\n Contents: {msg.page_content}\" for msg in msgs])\n",
        "\n",
        "chain = {\"context\": retriever | RunnableLambda(format_message), \"question\": RunnablePassthrough()} | conversation_template\n",
        "\n",
        "for message in chain.invoke(\"What is all I need?\").messages:\n",
        "  pprint((str(type(message)), message.content))"
      ],
      "metadata": {
        "id": "mpL56qEQbVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can even implement filters such as filtering out the `References`"
      ],
      "metadata": {
        "id": "_xSghm4cdBRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_message(msgs):\n",
        "  return '\\n\\n'.join([f\"Section: {next(iter(msg.metadata.values()))}\\n Contents: {msg.page_content}\" for msg in msgs if next(iter(msg.metadata.values())) != \"References\"])\n",
        "\n",
        "chain = {\"context\": retriever | RunnableLambda(format_message), \"question\": RunnablePassthrough()} | conversation_template\n",
        "\n",
        "for message in chain.invoke(\"What is this paper about?\").messages:\n",
        "  pprint((str(type(message)), message.content))"
      ],
      "metadata": {
        "id": "bOB6nhqIc7rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, present the conversation to the LLM and see what it responds"
      ],
      "metadata": {
        "id": "ZMW0jIJDdYxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = {\"context\": retriever | RunnableLambda(format_message), \"question\": RunnablePassthrough()} | conversation_template | llm | StrOutputParser()\n",
        "\n",
        "print(chain.invoke(\"What is this paper about?\"))"
      ],
      "metadata": {
        "id": "4sV0POmIdU_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can customize the prompt to get additional details, such as quoting the source of the information from which it drew the conclusion from"
      ],
      "metadata": {
        "id": "lhROnjiyd6we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_template = ChatPromptTemplate([\n",
        "    (\"system\", \"\"\"You will look at the following passages and reply to the question using only and only the information present in the following documents.\n",
        "         You will not refer to the information, just write the answer. After you give the answer, cite the section from which you read it.\n",
        "         If the information requested is not present in the documents or if the question can not be answered using the documents as context, just say that you can't answer the question\n",
        "         ```{context}```\n",
        "         \"\"\"),\n",
        "    (\"human\", \"\"\"Question: ```{question}```\n",
        "    Answer: \"\"\")\n",
        "])\n",
        "\n",
        "chain = {\"context\": retriever | RunnableLambda(format_message), \"question\": RunnablePassthrough()} | conversation_template | llm | StrOutputParser()\n",
        "\n",
        "print(chain.invoke(\"What is this paper about?\"))"
      ],
      "metadata": {
        "id": "LYHoJt2Ndp8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(chain.invoke(\"Give me the summary of this work\"))"
      ],
      "metadata": {
        "id": "eX_whHTxeSLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(chain.invoke(\"What are the elements of the transformer architecture?\"))"
      ],
      "metadata": {
        "id": "Xbkr8cTdeopL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Build a Gradio interface for our RAG pipeline"
      ],
      "metadata": {
        "id": "E3zZU9Yre-ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext gradio"
      ],
      "metadata": {
        "id": "GpJqdoY9e3Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%blocks\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# We need a function that calls our langchain pipeline\n",
        "\n",
        "def rag(question):\n",
        "  return chain.invoke(question)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=rag,             # Here, we are wiring the function to the interface\n",
        "    inputs=[\"text\"],      # Specify the input types\n",
        "    outputs=[\"text\"],     # Same, for output\n",
        ")"
      ],
      "metadata": {
        "id": "9tFDZwxcfd-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There you go.\n",
        "Each RAG application has its own details and nuances. There is no good \"one size fits all\" solution. Instead, it depends heavily in my design choices.\n",
        "\n",
        "This notebook provides a good starting point to implement your own RAG pipeline."
      ],
      "metadata": {
        "id": "ykaDZATKiW01"
      }
    }
  ]
}