
<img src="https://images.unsplash.com/photo-1655720855348-a5eeeddd1bc4?q=80&w=1932&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" width=860>

***

## Mastering Generative AI Foundation Models for Research

### Workshop Overview
Dive deep into the world of generative AI foundation models, exploring their transformative potential across scientific disciplines through a hands-on, accessible approach.

### Learning Objectives
By the end of this workshop, participants will:

* Develop a comprehensive understanding of generative AI foundation models
* Acquire practical skills for integrating AI technologies into research workflows
* Demonstrate proficiency in prompt engineering across multiple disciplines
* Critically evaluate and apply multimodal AI tools to complex research challenges
* Build confidence in navigating and deploying AI technologies
* Create innovative research approaches using generative AI methodologies

### Key Skills Developed

#### Advanced Prompt Engineering

* Crafting precise, context-specific prompts
* Extracting maximum value from foundation models
* Developing discipline-specific interaction strategies


#### Computational AI Infrastructure Management

* Understanding AI model architectures
* Managing computational resources
* Scaling AI applications from local to HPC environments


#### Multimodal AI Applications

* Integrating text, image, and data-based models
* Cross-modal research technique development
* Solving interdisciplinary research challenges


#### Practical AI Deployment Strategies

* Implementing AI tools in research workflows
* Performance optimization techniques
* Handling model limitations and biases


#### Code Generation and Optimization

* Utilizing AI for research code development
* Debugging and improving computational methods
* Automating repetitive research tasks


#### Ethical AI Implementation

* Recognizing and mitigating AI biases
* Ensuring research integrity
* Responsible AI use across disciplines


#### High-Performance Computing (HPC) Integration

* Deploying AI models in advanced computing environments
* Resource management and optimization
* Scaling computational research capabilities

### Core Focus: Foundation Models in Research

#### Understanding Foundation Models

* Explore large language models and multimodal AI systems
* Examine key models: GPT, BERT, DALL-E, Stable Diffusion
* Analyze model architectures, capabilities, and limitations
* Understand transfer learning and model adaptability

#### Generative AI as a Research Catalyst

* Bridging interdisciplinary research challenges
* Transforming data analysis and hypothesis generation
* Expanding computational research capabilities
* Democratizing advanced AI technologies

### Key Workshop Modules

#### 1. Prompt Engineering for Research

* Crafting effective prompts across disciplines
* Extracting maximum value from foundation models
* Developing discipline-specific interaction strategies
* Handling complex research queries

#### 2. Multimodal AI Applications

* Integrating text, image, and data-based models
* Cross-modal research techniques
* Practical implementation strategies
* Solving interdisciplinary research challenges

#### 3. Ethical AI and Responsible Use

* Understanding model biases
* Ensuring research integrity
* Responsible AI deployment
* Ethical considerations in AI-assisted research

#### 4. Computational Infrastructure

* Local to high-performance computing deployments
* Resource management strategies
* Scaling AI model applications
* Performance optimization techniques


### Target Audience

* Graduate students across all disciplines
* Researchers seeking AI integration
* Academics exploring computational technologies
* Interdisciplinary innovation seekers

### Learning Outcomes

* Confident foundation model utilization
* Advanced research methodology skills
* Computational thinking transformation
* Practical AI deployment capabilities

## Workshop Vision
Empowering researchers to leverage generative AI as a powerful, flexible research companion across scientific domains.


<!-- ## Workshop: Computational AI Mastery—From Local to Supercomputing -->

<!--
PREVIOUS Workshop description....
## Workshop: Mastering GenAI Applications

**Learning Objectives:**
Enable graduate students to master generative AI across computational environments, connecting theory with practical, scalable AI skills.

**Workshop Justification:**
Generative AI has become essential across all disciplines in today's rapidly advancing technological landscape. This workshop makes AI accessible to graduate students from diverse scientific backgrounds, regardless of their technical expertise.

**Why Grad Students Need This:**
Modern research demands technological adaptability. Generative AI represents a fundamental shift in research methodology, data analysis, and problem-solving. Through these tools, graduate students can:

- Accelerate research workflows
- Generate novel hypotheses
- Enhance computational capabilities
- Gain competitive advantages in academia and industry

**Key Skills Developed:**

1. Computational AI Infrastructure Management
2. Advanced Prompt Engineering
3. High-Performance Computing (HPC) Integration
4. Multimodal AI Applications
5. Practical AI Deployment Strategies
6. Code Generation and Optimization
7. Ethical AI Implementation

**Technical Prerequisites:**

- Basic programming knowledge (Python preferred)
- Fundamental understanding of command-line interfaces
- Curiosity and willingness to explore emerging technologies
- No advanced machine learning background required

**Major Advantages of Attending:**

- Convert complex computational challenges into manageable solutions
- Develop a practical toolkit for AI-enhanced research
- Master scalable approaches from local to high-performance computing
- Acquire interdisciplinary skills for any scientific domain

**Unique Workshop Approach:**
Unlike traditional technical training, we emphasize:

- Practical, immediately applicable skills
- Hands-on learning with real-world scenarios
- Flexibility across scientific disciplines
- Easy entry into AI technology

**Potential Impact:**
Participants will gain:

- Confidence with AI technologies
- Enhanced research methods
- Competitive skills for academia and industry
- Ability to innovate across computational domains

**Vision:**
This workshop goes beyond teaching technology—it opens doors to a new era of computational thinking. We empower the next generation of researchers to tackle complex problems with intelligent, scalable solutions.

**Call to Action:**
From biology to humanities, generative AI can transform your research. Join us to demystify and master computational AI at every scale.
-->
***

**Instructors**:  Nick Eddy / Carlos Lizárraga / Enrique Noriega/ Mithun Paul

* [**Registration**](https://uarizona.co1.qualtrics.com/jfe/form/SV_0wWiJ946ta9ExzE) to attend in person or online.

* **When**: Thursdays at 1PM.

* **Where**: Albert B. Weaver Science-Engineering Library. Room 212 

* **Zoom**: [https://arizona.zoom.us/j/89667081542](https://arizona.zoom.us/j/89667081542)


_(Program not definitive!)_

**Calendar**


***

### Spring 2025

| Date      | Title      | Topic Description     | Wiki/Slides      | YouTube | Instructor | 
| :--: | :-- | :--      | :--  | :--       | :--: | 
|  01/30/2025 | Scaling up Ollama: Local, CyVerse, HPC | In this hands-on workshop, participants will learn to deploy and scale large language models using Ollama across various computational environments—from laptops to supercomputing clusters—to master practical AI capabilities. | | [video](https://youtu.be/SuLc1bk3994?si=hc2tmI-L8ScnL-Eg)  | Enrique Noriega|
| 02/06/2025 | Using AI Verde  | This practical introduction shows how to effectively use U of A Generative AI Verde for academic research, writing, and problem-solving. Participants will learn to harness AI Verde's capabilities while gaining a clear understanding of its limitations and ethical implications. | | [video](https://youtu.be/M9Vos5LiwUo?si=8IBW006F7XNnA_gi)   | Nick Eddy |
|  02/13/2025| Best practices of Prompt Engineering using AI Verde | A hands-on session that teaches practical prompt engineering techniques to optimize U of A Generative AI Verde's performance for academic and professional applications. | [Slides](https://docs.google.com/presentation/d/1zx_6tRUo5m9Yz2nDOAg54di3bhHuW9KVx_7MQFI4yoE/edit?usp=sharing)  | [video](https://youtu.be/pVHeHTTNekY?si=ZxM4UAZq6ncIVtkZ) |Mithun Paul |
| 02/20/2025 | Quick RAG application using AI Verde / HPC | A hands-on session demonstrating how to build a basic Retrieval-Augmented Generation (RAG) system with the U of A Generative AI Verde API. Participants will learn to enhance AI responses by integrating custom knowledge bases. | [Slides](https://docs.google.com/presentation/d/1gMcJ348LLNRXgsXpFikIV_Fj-Cp5UMADzPOB_JJqdhA/edit?usp=sharing) | [video]() | Mithun Paul| 
| 02/27/2025 |  Multimodal Q&A+OCR in AI Verde | A hands-on technical session exploring U of A Generative AI's multimodal capabilities that combines vision and text processing for enhanced document analysis and automated question-answering with OCR technology. | | [video]() | Nick Eddy  |
| 03/06/2025 | SQL specialized query code generation |  A hands-on session teaching participants how to use Large Language Models to craft, optimize, and validate complex SQL queries, emphasizing real-world database operations and industry best practices. |  | [video]() | Enrique Noriega |
| 03/13/2025  | **NO Session** | **Spring Break** | | |
| 03/20/2025 | Function calling with LLMs | There are two ways to implement function calling with open-source large language models (LLMs). When an LLM doesn't natively support function calling, you can combine prompt engineering, fine-tuning, and constrained decoding. | | [video]() | Enrique Noriega | 
| 03/27/2025 | Code generation assistants |  Large Language Models (LLMs) now serve as powerful code generation assistants, streamlining and enhancing software development. They generate code snippets, propose solutions, and translate code between programming languages.| | [video]() | Nick Eddy  | 


***

# Fall 2024

| Date      | Title      | Topic Description         | YouTube | Instructor |  
|:--: | :-- | :--         | :--: | :-- | 
| 09/05/2024 | [Hugging Face Models (NLP)](https://github.com/ua-datalab/Generative-AI/wiki/NLP-with-Hugging-Face-Transformers) | Hugging Face offers a vast array of pre-trained models for Natural Language Processing (NLP) tasks. These models cover a wide spectrum of applications, from text generation and translation to sentiment analysis and question answering. |[video](https://youtu.be/IJn9r6pJykw?si=3pOob4Iwnucjp50Y)  |  Enrique Noriega|
| 09/12/2024 | [Hugging Face Models (Computer Vision)](https://github.com/ua-datalab/Generative-AI/wiki/Computer-Vision-with-Hugging-Face-Transformers)| Hugging Face has significantly expanded its offerings beyond NLP to encompass a robust collection of computer vision models. You can find pre-trained models for a wide range of tasks, from basic image classification to complex image generation.  | [video](https://youtu.be/uNXeV-uQdOo?si=Lqn-FYXEb700cP6q)  | Enrique Noriega |
| 09/19/2024  | [Hugging Face Models (Multimodal)](https://github.com/ua-datalab/Generative-AI/wiki/Multimodal-LLM-with-Hugging-Face-Transformers)|  Hugging Face offers a diverse range of multimodal models, capable of processing and understanding multiple data modalities such as text, images, and audio. These models are at the forefront of AI research and development, enabling innovative applications.  | [video](https://youtu.be/99Nb6XLOwqE?si=M-L0-tSlQRD6wmPE)  | Enrique Noriega  |
| 09/26/2024  | [Running LLM locally: Ollama](https://github.com/ua-datalab/Generative-AI/wiki/Running-LLM-Locally:-Ollama)  | Ollama is an open-source platform designed to make running large language models (LLMs) on your local machine accessible and efficient. It acts as a bridge between the complex world of LLMs and users who want to experiment and interact with these models without relying on cloud-based services.  | [video](https://youtu.be/Ywi57myR-Q8?si=F5_T0cxOQQIoconA)  | Carlos Lizárraga|
| 10/03/2024  |  [Introduction to LangChain](https://github.com/ua-datalab/Generative-AI/wiki/Introduction-to-Langchain) | Langchain is an open-source Python library that provides a framework for developing applications powered by large language models (LLMs). It simplifies the process of building complex LLM-based applications by offering tools and abstractions to connect LLMs with other data sources and systems. | [video](https://youtu.be/EWOJwNoeIa0?si=VV3OpSzekEUqj6EN)  | Enrique Noriega |
| 10/10/2024  | [Getting Started with Phi-3](https://github.com/ua-datalab/Generative-AI/wiki/Getting-started-with-Phi%E2%80%903) | Phi-3 is a series of small language models (SLMs) developed by Microsoft. Unlike larger language models (LLMs) that require substantial computational resources, Phi-3 models offer impressive performance while being significantly smaller and more efficient.| [video](https://youtu.be/lo8Zdytfx9I?si=WUYZff5Dl5S2j5C_)  |  Enrique Noriega |
| 10/17/2024 | [Getting started with Gemini](https://github.com/ua-datalab/Generative-AI/wiki/Getting-started-with-Gemini)  | Gemini is a large language model (LLM) developed by Google AI. It's designed to be exceptionally versatile, capable of handling a wide range of tasks and modalities, including text, code, audio, and images. This makes it a significant advancement in the field of artificial intelligence. | [video](https://youtu.be/kVP7hI5PxX0?si=99ZbPvRzuum4WWhG)  | Enrique Noriega |
| 10/24/2024 | [Introduction to Gradio](https://github.com/ua-datalab/Generative-AI/wiki/Introduction-to-Gradio) | Gradio is an open-source Python library that allows you to quickly create user interfaces for your machine learning models, APIs, or any Python function. It simplifies the process of building interactive demos and web applications without requiring extensive knowledge of JavaScript, CSS, or web development.| [video](https://youtu.be/JHIKpScQcg8?si=MF4KTeBoOFgbYvKg)  | Enrique Noriega |
| 10/31/2024 | [Introduction to RAG](https://github.com/ua-datalab/Generative-AI/wiki/Introduction-to-RAG) | Retrieval-Augmented Generation. It's a technique that enhances the capabilities of Large Language Models (LLMs) by combining them with external knowledge sources.  | [video](https://youtu.be/5pnl0Y64JFQ?si=MmGsD8gTKQfKlzF5)  | Enrique Noriega |
| 11/15/2024 | Dense Passage Retrieval  |  | [video](https://youtu.be/xY010hbBnG4?si=nHE9QnjEXg4yTwmj) | Mithun Paul |  

****
Created: 06/10/2024 (C. Lizárraga)

Updated: 02/24/2025 (C. Lizárraga)

[DataLab](https://www.datascience.arizona.edu/education/uarizona-data-lab), Data Science Institute,
University of Arizona. 


<img src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png" width="128">  [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)

[<img src="https://datascience.arizona.edu/sites/default/files/Data%20Science%20Institute_Webheader%20%281%29.svg" width="256">](https://datascience.arizona.edu)
