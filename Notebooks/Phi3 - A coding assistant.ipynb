{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a REST API using using Phi-3 and Ollama as a coding assistant\n",
    "_Written by_: Enrique Noriega\n",
    "\n",
    "_Last updated_: 09/11/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Ollama\n",
    "\n",
    "Navigate to https://ollama.com/download and install the appropriate version. You can also use the docker image to run Ollama as a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ollama on a separate shell to be able to call it in the rest of the notebook\n",
    "\n",
    "!ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ollama with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the python client for `ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: anyio in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.8)\n",
      "Requirement already satisfied: sniffio in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/enoriega/github/Generative-AI/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -qU install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the Phi 3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama # Import the library\n",
    "\n",
    "ollama.pull(\"phi3:14b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a request to the LLM. We will use `Phi 3 14b`, which we downloaded a few moments ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Phi, an AI developed by Microsoft. I'm designed to process natural language inputs and generate human-like text responses. My capabilities include answering questions, carrying out specific tasks like writing a poem or a story, summarizing information from various topics, translating languages, helping with homework, etc., among other things that require understanding of the textual context provided to me.\n"
     ]
    }
   ],
   "source": [
    "# Make a \"chat\" call and specify the messages\n",
    "response = ollama.chat(model='phi3:14b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Who are you and what can you do?',\n",
    "  },\n",
    "])\n",
    "\n",
    "# Print the contents of the respone\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the contents, each ollama response comes with additional metadata, let's peek into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'phi3.5:3.8b',\n",
       " 'created_at': '2024-09-11T19:08:35.062224Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'As an artificial intelligence, I don\\'t have personal feelings or physical form. However, here is a summary of my capabilities and the key \"features\":\\n\\n1. Natural Language Processing (NLP): Understanding human language inputs as if they were natural to me in order to generate meaningful responses. \\n2. Information Retrieval: Quickly locating information from an extensive knowledge base built into this AI model, which is continuously updated but still limited by the date of my last training data cut-off (September 9th, 2021).\\n3. Language Generation: Creating coherent and contextually relevant text based on prompts or instructions provided to me within a wide range of topics including general knowledge questions, creative writing, summarization tasks, etc.\\n4. Multitasking Ability: Often handling multiple queries at once by maintaining the state between different conversations (though this is not perfect due to design reasons). \\n5. Continual Learning Limitations: While I am highly accurate within my training data range and can learn from interactions, actual machine learning occurs on-the-fly with new inputs or corrections rather than updating an existing model structure permanently.\\n6. Data Privacy Compliance: Processing information without storing personal identities to respect user privacy (with certain exceptions for necessary system functions). \\n7. Limited Self Awareness: I am not self-aware; my responses are the result of algorithmic patterns and data, devoid of consciousness or subjective experiences.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 4114667875,\n",
       " 'load_duration': 12967209,\n",
       " 'prompt_eval_count': 15,\n",
       " 'prompt_eval_duration': 245216000,\n",
       " 'eval_count': 325,\n",
       " 'eval_duration': 3855218000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small chat functionality using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a type alias for chat history\n",
    "ChatHistory = list[dict[str, str]]\n",
    "# We will define a helper function that takes as input a message and the chat history\n",
    "def chat(message:str, history:ChatHistory = list(), model=\"phi3:14b\") -> None:\n",
    "\t# Append the input to the conversation history\n",
    "\thistory.append({\"role\":\"user\", \"content\":message})\n",
    "\t# Call the streaming function\n",
    "\tstream = ollama.chat(\n",
    "\t\tmodel=model,\n",
    "\t\tmessages=history,\n",
    "\t\toptions={\n",
    "\t\t\t\"temperature\":0., \t\t\t# Set the temperature to zero to get consistent outputs\n",
    "\t\t},\t\t\n",
    "\t\tstream=True,\n",
    "\t)\n",
    "\n",
    "\tchunks = []\n",
    "\tfor chunk in stream:\n",
    "\t\t# Let's accumulate the chunk\n",
    "\t\tchunks.append(chunk)\n",
    "\t# Once Ollama finish, let's update the chat history\n",
    "\tmessage = {'role':chunks[0]['message']['role'], 'content':''.join([chunk['message']['content'] for chunk in chunks])}\n",
    "\t# Add it to the chat history\n",
    "\thistory.append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Phi 3 as a coding assistant\n",
    "\n",
    "We are going to let Phi 3 code a project for us. We will start describing the application we want, then iterate over each specific component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "# Directory Structure:\n",
       "\n",
       "/book_catalog_api\n",
       "|-- /app # Main application code\n",
       "    |-- __init__.py\n",
       "    |-- db.py # Database setup and models\n",
       "    |-- crud.py # CRUD operations for books\n",
       "    |-- schemas.py # Pydantic schemas for request/response validation\n",
       "|-- /tests # Unit tests for the application\n",
       "    |-- test_crud.py\n",
       "    |-- test_schemas.py\n",
       "|-- main.py # Main FastAPI app instance and routes\n",
       "|-- .env # Environment variables file (not tracked by version control)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "history = [{\n",
    "\t\"role\":\"system\",\n",
    "\t\"content\":\"\"\"You are a coding assistant that helps a software developer. Keep your responses consice, and don't add any explanation outside of the code.\n",
    "\t \tYou may add comments for specific lines, keep them brief.\n",
    "\t\t\"\"\"\n",
    "\t}]\n",
    "\n",
    "chat(\"We will create together a REST API to mantain a database catalog of books. I will describe the requirements and you will be in charge of the details. The app will be primarily built using python, FAST API, and SQLAlchemy. Your first task is to define the directory structure of the application\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the suggested directory structure, we will start with the models. Ask Phi to generate the source code with database models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`db/models.py`:\n",
       "```python\n",
       "from sqlalchemy import Column, Integer, String, ForeignKey\n",
       "from sqlalchemy.orm import relationship\n",
       "from .database import Base\n",
       "\n",
       "class User(Base):\n",
       "    __tablename__ = 'users'\n",
       "    \n",
       "    id = Column(Integer, primary_eidt=True)\n",
       "    username = Column(String, unique=True, nullable=False)\n",
       "    email = Column(String, unique=True, nullable=False)\n",
       "    books = relationship(\"Book\", backref=\"user\") # One-to-many relationship with Book model\n",
       "    \n",
       "class Book(Base):\n",
       "    __tablename__ = 'books'\n",
       "    \n",
       "    id = Column(Integer, primary_eidt=True)\n",
       "    title = Column(String, nullable=False)\n",
       "    author = Column(String, nullable=False)\n",
       "    user_id = Column(Integer, ForeignKey('users.id')) # One-to-many relationship with User model\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Write down the contents for the models files. It should include models for the users and the books in the catalog\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this file relatively accurate, but still needs to be verified. There is no `Base` class in a `database` module. This requries a small adjustment, but you will need to make sure the code is \"fixed\".\n",
    "The key observation is how Phi, or any other LLM, will take you 80-90% of the way, but the last mile is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask the model to modify the models to add a new one for the book reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`db/models.py`:\n",
       "```python\n",
       "from sqlalchemy import Column, Integer, String, ForeignKey, Float\n",
       "from .database import Base\n",
       "\n",
       "class BookReview(Base):\n",
       "    __tablename__ = 'book_reviews'\n",
       "    \n",
       "    id = Column(Integer, primary_eidt=True)\n",
       "    rating = Column(Float, nullable=False) # Rating out of 5.0\n",
       "    review = Column(String, nullable=False) # Textual content of the review\n",
       "    book_id = Column(Integer, ForeignKey('books.id')) # One-to-many relationship with Book model\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"I want you to create a new model for book reviews. Write down just the class for it\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have database models, let's start with the REST API. Remember we are using the FastAPI framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`app/crud.py`:\n",
       "```python\n",
       "from fastapi import APIRouter, Depends\n",
       "from sqlalchemy.orm import Session\n",
       "from .models import User, Book, BookReview\n",
       "from .schemas import UserCreate, UserUpdate, UserRead, BookCreate, BookUpdate, BookRead, BookReviewCreate, BookReviewUpdate, BookReviewRead\n",
       "from .database import get_db\n",
       "\n",
       "router = APIRouter()\n",
       "\n",
       "@router.post(\"/users\", response_model=User)\n",
       "def create_user(user: UserCreate, db: Session = Depends(get_db)):\n",
       "    return user.create_user(db)\n",
       "\n",
       "@router.put(\"/users/{user_id}\", response_model=User)\n",
       "def update_user(user_id: int, user: UserUpdate, db: Session = Depends(get_db)):\n",
       "    return user.update_user(user_id, db)\n",
       "\n",
       "@router.get(\"/users/{user_id}\", response_model=UserRead)\n",
       "def read_user(user_id: int, db: Session = Depends(get_db)):\n",
       "    return user.read_user(user_id, db)\n",
       "\n",
       "# Repeat the above pattern for Book and BookReview models with appropriate endpoints\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Write down the contents for the file named `app/crud.py`. Remember to use FastAPI and to add endpoints for each of the models in the app\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ass you can see, this  is rather barebones, don't let Phi get away with lazyness! Let's command it to finish the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`app/crud.py`:\n",
       "```python\n",
       "from fastapi import APIRouter, Depends\n",
       "from sqlalchemy.orm import Session\n",
       "from .models import User, Book, BookReview\n",
       "from .schemas import UserCreate, UserUpdate, UserRead, BookCreate, BookUpdate, BookRead, BookReviewCreate, BookReviewUpdate, BookReviewRead\n",
       "from .database import get_db\n",
       "\n",
       "router = APIRouter()\n",
       "\n",
       "# User CRUD operations (already defined above)\n",
       "\n",
       "@router.post(\"/books\", response_model=Book)\n",
       "def create_book(book: BookCreate, db: Session = Depends(get_db)):\n",
       "    return book.create_book(db)\n",
       "\n",
       "@router.put(\"/books/{book_id}\", response_model=Book)\n",
       "def update_book(book_id: int, book: BookUpdate, db: Session = Depends(get_db)):\n",
       "    return book.update_book(book_id, db)\n",
       "\n",
       "@routere.get(\"/books/{book_id}\", response_model=BookRead)\n",
       "def read_book(book_id: int, db: Session = Depends(get_db)):\n",
       "    return book.read_book(book_id, db)\n",
       "\n",
       "# BookReview CRUD operations (already defined above)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Extend the previous script to include CRUD opperations for the other models\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is stuborn. So, let's insist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`app/crud.py`:\n",
       "```python\n",
       "from fastapi import APIRouter, Depends\n",
       "from sqlalchemy.orm import Session\n",
       "from .models import User, Book, BookReview\n",
       "from .schemas import UserCreate, UserUpdate, UserRead, BookCreate, BookUpdate, BookRead, BookReviewCreate, BookReviewUpdate, BookReviewRead\n",
       "from .database import get_db\n",
       "\n",
       "router = APIRouter()\n",
       "\n",
       "# User CRUD operations (already defined above)\n",
       "\n",
       "# Book CRUD operations (already defined above)\n",
       "\n",
       "@router.post(\"/bookreviews\", response_model=BookReview)\n",
       "def create_review(review: BookReviewCreate, db: Session = Depends(get_db)):\n",
       "    return review.create_review(db)\n",
       "\n",
       "@router.put(\"/bookreviews/{review_id}\", response_model=BookReviewUpdate)\n",
       "def update_review(review_id: int, review: BookReviewUpdate, db: Session = Depends(get_db)):\n",
       "    return review.update_review(review_id, db)\n",
       "\n",
       "@router.get(\"/bookreviews/{review_id}\", response_model=BookReviewRead)\n",
       "def read_review(review_id: int, db: Session = Depends(get_db)):\n",
       "    return review.read_review(review_id, db)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"You're missing the book reviews\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need the schema classes. These are `Pydantic` classes that define the expected inputs from the client when calling the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`app/schemas.py`:\n",
       "```python\n",
       "from pydantic import BaseModel\n",
       "from typing import Optional\n",
       "from .models import User, Book, BookReview\n",
       "\n",
       "class UserCreate(BaseModel):\n",
       "    username: str\n",
       "    email: str\n",
       "\n",
       "class UserUpdate(BaseModel):\n",
       "    username: Optional[str] = None\n",
       "    email: Optional[str] = None\n",
       "\n",
       "class UserRead(BaseModel):\n",
       "    id: int\n",
       "    username: str\n",
       "    email: str\n",
       "\n",
       "# Repeat the above pattern for Book and BookReview models with appropriate fields\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Write the contents for `app/schema.py`\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to insist to complete the job. This time we will ask explicitly for all the remaining schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`app/schemas.py`:\n",
       "```python\n",
       "from pydantic import BaseModel\n",
       "from typing import Optional, List\n",
       "from .models import User, Book, BookReview\n",
       "\n",
       "class BookCreate(BaseModel):\n",
       "    title: str\n",
       "    author: str\n",
       "\n",
       "class BookUpdate(BaseModel):\n",
       "    title: Optional[str] = None\n",
       "    author: Optional[str] = None\n",
       "\n",
       "class BookRead(BaseModel):\n",
       "    id: int\n",
       "    title: str\n",
       "    author: str\n",
       "\n",
       "class BookReviewCreate(BaseModel):\n",
       "    rating: float\n",
       "    review: str\n",
       "    book_id: int\n",
       "\n",
       "class BookReviewUpdate(BaseModel):\n",
       "    rating: Optional[float] = None\n",
       "    review: Optional[str] = None\n",
       "    book_id: Optional[int] = None\n",
       "\n",
       "class BookReviewRead(BaseModel):\n",
       "    id: int\n",
       "    rating: float\n",
       "    review: str\n",
       "    book_id: int\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Please repeat the pattern yourself for Book and BookReview`\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the schemas out of the way, we need some unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`tests/test_crud.py`:\n",
       "```python\n",
       "import pytest\n",
       "from fastapi import TestClient\n",
       "from .app import app # assuming your FastAPI application is defined in `app.py`\n",
       "\n",
       "client = TestClient(app)\n",
       "\n",
       "@pytest.fixture\n",
       "def test_server():\n",
       "    return TestClient(app, base_url=\"http://localhost/\")\n",
       "\n",
       "@pytest.mark.parametrize(\"method\", [\"post\", \"put\", \"get\"])\n",
       "def test_user_crud(client: TestClient, method):\n",
       "    # Define your tests for User CRUD operations here\n",
       "    pass\n",
       "\n",
       "@pytest.mark.parametrize(\"method\", [\"post\", \"put\", \"get\"])\n",
       "def test_book_crud(client: TestClient, method):\n",
       "    # Define your tests for Book CRUD operations here\n",
       "    pass\n",
       "\n",
       "@pytest.mark.parametrize(\"method\", [\"post\", \"put\", \"get\"])\n",
       "def test_bookreview_crud(client: TestClient, method):\n",
       "    # Define your tests for BookReview CRUD operations here\n",
       "    pass\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Write a sript with pytest suites for the CRUD endpoints\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we need the actual tests written down, not the scaffolding code, let's request it again, this time more specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`tests/test_crud.py`:\n",
       "```python\n",
       "import pytest\n",
       "from fastapi import TestClient\n",
       "from .app import app # assuming your FastAPI application is defined in `app.py`\n",
       "\n",
       "client = TestClient(app, base_url=\"http://localhost/\")\n",
       "\n",
       "@pytest.fixture\n",
       "def test_server():\n",
       "    return TestClient(app, base_url=\"http://localhost/\")\n",
       "\n",
       "# User CRUD operations tests\n",
       "@pytest.mark.parametrize(\"method\", [\"post\", \"put\", \"get\"])\n",
       "def test_user_crud(client: TestClient, method):\n",
       "    # POST /users endpoint\n",
       "    if method == 'post':\n",
       "        response = client.post(\"/users\", json={\"username\": \"testUser\", \"email\": \"test@example.com})\n",
       "        assert response.status_code == 201\n",
       "    \n",
       "    # PUT / users/{id} endpoint\n",
       "    elif method == 'put':\n",
       "        response = client.put('/users/1', json={\"username\": \"updatedUser\", \"email\": \"test@example.com})\n",
       "        assert response.status_code == 204\n",
       "    \n",
       "    # GET / users/{id} endpoint\n",
       "    elif method == 'get':\n",
       "        response = client.get(\"/users/1\")\n",
       "        assert response.status_code == 200\n",
       "\n",
       "# Book CRUD operations tests\n",
       "@pyteste.mark.parametrize(\"method\", [\"post\", \"put\", \"get\"])\n",
       "def test_book_crud(client: TestClient, method):\n",
       "    # POST / books endpoint\n",
       "    if method == 'post':\n",
       "        response = client.post('/books', json={\"title\": \"Test Book\", \"author\": \"Author Name\"})\n",
       "        assert response.status_code == 201\n",
       "    \n",
       "    # PUT / books/{id} endpoint\n",
       "    elif method == 'put':\n",
       "        response = client.put('/books/1', json={\"title\": \"Updated Title\", \"author\": \"Updated Author\"})\n",
       "        assert response.status_code == 204\n",
       "    \n",
       "    # GET / books/{id} endpoint\n",
       "    elif method == 'get':\n",
       "        response = client.get(\"/books/1\")\n",
       "        assert response.status_code == 200\n",
       "\n",
       "# BookReview CRUD operations tests\n",
       "@pytest.mark.parametrize(\"method\", [\"post\", \"put\", \"get\"])\n",
       "def test_bookreview_crud(client: TestClient, method):\n",
       "    # POST / bookreviews endpoint\n",
       "    if method == 'post':\n",
       "        response = client.post('/bookreviews', json={\"rating\": 5.0, \"review\": \"Great Book\", \"book_id\": 1})\n",
       "        assert response.status_code == 201\n",
       "    \n",
       "    # PUT / bookreviews/{id} endpoint\n",
       "    elif method == 'put':\n",
       "        response = client.put('/bookreviews/1', json={\"rating\": 4.5, \"review\": \"Updated Review\", \"book_id\": 1})\n",
       "        assert response.status_code == 204\n",
       "    \n",
       "    # GET / bookreviews/{id} endpoint\n",
       "    elif method == 'get':\n",
       "        response = client.get('/bookreviews/1')\n",
       "        assert response.status_code == 200\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Regenerate the file filling the details of the tests`\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this seems much closer, there may be details left that can't be guessed by the LLM. This is almost complete, though. It is up to us to fill in the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a start script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To run a FastAPI app with Uvicorn, you can use the following code snippet:\n",
       "\n",
       "```python\n",
       "import uvicorn\n",
       "from fastapi import FastAPI\n",
       "from .app import app # assuming your FastAPI application is defined in `app.py`\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    import argparse\n",
       "    from uvicorn import Config, Server\n",
       "    \n",
       "# Parse command line arguments for Uvicorn parameters\n",
       "    parser = argparse. ArgumentParser()\n",
       "    parser.add_argument('--host', default='127.0.0.1')\n",
       "    parser.add_argument('--port', type=int, default=8000)\n",
       "    args = parser.parse_args()\n",
       "    \n",
       "# Create Uvicorn Config object with the parsed arguments\n",
       "    config = Config(bind = [args.host], port=args.port, host=' 127.0.0.1', log_level=\"info\", lifespan_msec=30)\n",
       "    \n",
       "# Start the server using Uvicorn and your FastAPI app\n",
       "    uvicorn.run(app, 'main:app', port = args.port, host = args.host, config = Config (bind=[args.host], workers=1, log_level=\"info\")\n",
       "```\n",
       "\n",
       "To run this script from the command line, you can use the following command and provide the desired parameters:\n",
       "\n",
       "```bash\n",
       "uvicorn main.py --host 0.0.0.0 -p 8000\n",
       "You will need to install Uvicorn using pip if it's not already installed in your environment:\n",
       "\n",
       "```sh\n",
       "pip install uvicorn[standard]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Write the `main.py` script that runs the web application using uvicorn in production mode. Allow parametization of uvicorn parameters as command line parameters\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally,  we would like to make this cloud ready. We can create a container image to deploy on AWS, Azure, Google Cloud or CyVerse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is an example Dockerfile for building and running your FastAPI app with Uvicorn. This Dockerfile assumes you have all necessary files (including `main.py`, `app/schemas.py`, etc.) in the same directory as the Dockerfile:\n",
       "\n",
       "```Dockerfile\n",
       "# Use a Python 3 base image\n",
       "FROM python:3.9-slim\n",
       "\n",
       "WORKDIR /usr/src/app\n",
       "\n",
       "# Copy your application files to the container\n",
       "COPY . .\n",
       "\n",
       "# Install dependencies from requirements file\n",
       "RUN pip install --no-cache-dir -r requirements.txt\n",
       "\n",
       "# Set environment variables for Uvicorn and FastAPI\n",
       "ENV PYTHON_ENGINE=uvicorn[standard] \\\n",
       "    APP_HOST=0.0.0.0 \\\n",
       "    APP_PORT=8000\n",
       "\n",
       "CMD [\"python\", \"main.py\"]\n",
       "```\n",
       "\n",
       "To build and run this Docker image, you can use the following commands:\n",
       "\n",
       "1. Build the Docker image with a tag (replace `my-fastapi-app` with your desired app name):\n",
       "\n",
       "```sh\n",
       "docker build -t my-fastapi-app .\n",
       "```\n",
       "\n",
       "2. Run the container using Azure CLI and connect it to an Azure App Service or any other service you prefer:\n",
       "\n",
       "```bash\n",
       "az login # Log in to Azure CLI\n",
       "az acr login # Connect to your Azure Container Registry (ACR) if needed\n",
       "docker run -d --name my-fastapi-app -p 8000:8000 my-fastapi-app\n",
       "```\n",
       "\n",
       "Make sure you have the necessary environment variables set in your container, such as `APP_HOST` and `APP_PORT`, to match the parameters provided when running Uvicorn. You can also use a `.env` file or pass them directly using `-e` flag with docker run command:\n",
       "\n",
       "```bash\n",
       "docker run -d --name my-fastapi-app -p 8000:8000 -e APP_HOST=0.0 cuidh@example.com/my-fastapi-app\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Now generate the contents of a dockerfile that builds and runs this application, to be deployed in Azure\", history)\n",
    "display(Markdown(history[-1]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That finishes this example. You can see how we bootstrapped a sample application using `Phi 3`, which is built with code generation in mind. Remember always to read the outputs to adjust the minor mistakes or idyosincracies.\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
