{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nD1ytD8dBbuV",
        "outputId": "e4f9162d-bafa-4b52-dc61-e6cc4ac277bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "\n",
        "%pip install transformers datasets tokenizers torch sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs: Tokenizing\n",
        "\n",
        "The first step towards using NLP transformer models is to tokenize the inputs.\n",
        "\n",
        "Each model is paired with a tokenizer model. They go hand-in-hand because the initial embeddings are tied to the vocabulary supported by the tokenizer.\n",
        "\n",
        "Fortunately, we can fetch the appropriate tokenizer with the model name"
      ],
      "metadata": {
        "id": "and4OZtTKeZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We leverage the auto classes heavily\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Input sequence\n",
        "input_sequence = \"Hello, how are you today?\"\n",
        "\n",
        "# Encode the input sequence\n",
        "token_ids = tokenizer.encode(input_sequence, add_special_tokens=True)\n",
        "\n",
        "# Print the token IDs\n",
        "print(\"Token ids:\", token_ids)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(token_ids))\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Decoded input:\", tokenizer.decode(token_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAmcnHLtGdY7",
        "outputId": "fbdf50ce-a017-4202-b266-f31e617b3a80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ids: [101, 7592, 1010, 2129, 2024, 2017, 2651, 1029, 102]\n",
            "Tokens: ['[CLS]', 'hello', ',', 'how', 'are', 'you', 'today', '?', '[SEP]']\n",
            "Decoded input: [CLS] hello, how are you today? [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try another sequence\n",
        "input_sequence = \"The One Ring, forged in the fires of Mount Doom, holds immense power and corrupts those who wield it, leading to the downfall of many kingdoms.\"\n",
        "# Tokenize the input sequence\n",
        "tokens = tokenizer.tokenize(input_sequence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens to token IDs\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A33jO053Jk3l",
        "outputId": "1f7f1f4b-5bf6-4ef2-eaea-c8f32c8c988b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['the', 'one', 'ring', ',', 'forged', 'in', 'the', 'fires', 'of', 'mount', 'doom', ',', 'holds', 'immense', 'power', 'and', 'corrupt', '##s', 'those', 'who', 'wi', '##eld', 'it', ',', 'leading', 'to', 'the', 'downfall', 'of', 'many', 'kingdoms', '.']\n",
            "Token IDs: [1996, 2028, 3614, 1010, 16158, 1999, 1996, 8769, 1997, 4057, 12677, 1010, 4324, 14269, 2373, 1998, 13593, 2015, 2216, 2040, 15536, 14273, 2009, 1010, 2877, 2000, 1996, 22252, 1997, 2116, 12028, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers usually require more than just the token identifiers. The tokenizer will help you marshall the data appropriately"
      ],
      "metadata": {
        "id": "enDkTe4xYQhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the input for the transformer\n",
        "from pprint import pprint\n",
        "input = tokenizer(\"I'm so excited about my new job!\")\n",
        "pprint(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeFTjvUHLbHb",
        "outputId": "c509fcca-70f2-4959-8f40-50a8223ad437"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            " 'input_ids': [101,\n",
            "               1045,\n",
            "               1005,\n",
            "               1049,\n",
            "               2061,\n",
            "               7568,\n",
            "               2055,\n",
            "               2026,\n",
            "               2047,\n",
            "               3105,\n",
            "               999,\n",
            "               102],\n",
            " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding the input with a model\n",
        "\n",
        "The next step is actually running a model with the input produced by a tokenizer.\n",
        "\n",
        "Let's try a sentiment classification model. It will predict whether a phrase's sentiment is either `positive`, `neutral` or `negative`."
      ],
      "metadata": {
        "id": "Fa-eddIILx5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "# Model name from HuggingFace's Hub\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# See the configuration\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the input\n",
        "# text = \"I'm so excited about my new job!\"\n",
        "# text = \"I'm feeling really down today.\"\n",
        "text = \"I'm going to the store.\"\n",
        "input = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# See how roberta tokenizer doesn't add the token types\n",
        "pprint(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vxF8Uz-MAUH",
        "outputId": "f4a64808-cc3f-424f-fa6f-e3da2638d0ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
            " 'input_ids': tensor([[   0,  100,  437,  164,    7,    5, 1400,    4,    2]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the transformer"
      ],
      "metadata": {
        "id": "ITHHETdwZWS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# Run the transformer\n",
        "output = model(**input)\n",
        "pprint(output)\n",
        "\n",
        "# Normalize them to represent probabilities\n",
        "scores = output[0][0].detach().numpy()\n",
        "scores = softmax(scores)\n",
        "\n",
        "print(\"Probabilities:\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJOsHmV3MqYX",
        "outputId": "43fefea1-d72a-4769-a731-de52fb4a4cdf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None,\n",
            "                         logits=tensor([[-1.9199,  1.5211,  0.1629]], grad_fn=<AddmmBackward0>),\n",
            "                         hidden_states=None,\n",
            "                         attentions=None)\n",
            "Probabilities: [0.02484628 0.77571774 0.19943592]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to go from probabilities to chosing a predicted label"
      ],
      "metadata": {
        "id": "3gc3i-2jZg63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sense of the output\n",
        "ranking = np.argsort(scores)\n",
        "ranking = ranking[::-1]   # Reverse the elements\n",
        "# Print each class' label and score\n",
        "for i in range(scores.shape[0]):\n",
        "    l = config.id2label[ranking[i]]\n",
        "    s = scores[ranking[i]]\n",
        "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr3WaObRM86x",
        "outputId": "1c3852c7-6842-470a-b74b-b3982890b9a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) neutral 0.7757\n",
            "2) positive 0.1994\n",
            "3) negative 0.0248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamline using pipelines\n",
        "\n",
        "Using transformers can be a bit involved. Pipelines allow us to streamline the interface by abstracting away the details of each \"task\""
      ],
      "metadata": {
        "id": "PdzEBVn0OFAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools as it\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained transformer model for sequence classification\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for text classification\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Examples of spam and not spam texts\n",
        "positive_tweets = [\n",
        "    \"I'm so excited about my new job!\",\n",
        "    \"This is the best pizza I've ever had!\",\n",
        "    \"I love spending time with my family.\",\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"I just got a promotion!\"\n",
        "]\n",
        "\n",
        "negative_tweets = [\n",
        "    \"I'm so tired and stressed.\",\n",
        "    \"I hate traffic jams.\",\n",
        "    \"My computer is broken.\",\n",
        "    \"I lost my keys.\",\n",
        "    \"I'm feeling really down today.\"\n",
        "]\n",
        "\n",
        "neutral_tweets = [\n",
        "    \"It's raining outside.\",\n",
        "    \"I'm watching a movie.\",\n",
        "    \"I just ate lunch.\",\n",
        "    \"I'm going to the store.\",\n",
        "    \"I'm listening to music.\"\n",
        "]\n",
        "\n",
        "# Classify the texts\n",
        "for text in it.chain(positive_tweets, negative_tweets, neutral_tweets):\n",
        "    result = classifier(text)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Label: {result[0]['label']}, Score: {result[0]['score']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtUhEJEoB2E9",
        "outputId": "385d3b07-632f-403b-c92d-0d5fa57a09d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I'm so excited about my new job!\n",
            "Label: positive, Score: 0.9876318573951721\n",
            "\n",
            "Text: This is the best pizza I've ever had!\n",
            "Label: positive, Score: 0.9853105545043945\n",
            "\n",
            "Text: I love spending time with my family.\n",
            "Label: positive, Score: 0.9865761995315552\n",
            "\n",
            "Text: The weather is beautiful today.\n",
            "Label: positive, Score: 0.9844391942024231\n",
            "\n",
            "Text: I just got a promotion!\n",
            "Label: positive, Score: 0.9725865721702576\n",
            "\n",
            "Text: I'm so tired and stressed.\n",
            "Label: negative, Score: 0.8532361388206482\n",
            "\n",
            "Text: I hate traffic jams.\n",
            "Label: negative, Score: 0.9240683913230896\n",
            "\n",
            "Text: My computer is broken.\n",
            "Label: negative, Score: 0.8859074711799622\n",
            "\n",
            "Text: I lost my keys.\n",
            "Label: negative, Score: 0.6730405688285828\n",
            "\n",
            "Text: I'm feeling really down today.\n",
            "Label: negative, Score: 0.9055712223052979\n",
            "\n",
            "Text: It's raining outside.\n",
            "Label: neutral, Score: 0.6317095756530762\n",
            "\n",
            "Text: I'm watching a movie.\n",
            "Label: neutral, Score: 0.7492241859436035\n",
            "\n",
            "Text: I just ate lunch.\n",
            "Label: neutral, Score: 0.5950841307640076\n",
            "\n",
            "Text: I'm going to the store.\n",
            "Label: neutral, Score: 0.7757177352905273\n",
            "\n",
            "Text: I'm listening to music.\n",
            "Label: neutral, Score: 0.7014932036399841\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching by using a GPU\n",
        "\n",
        "Leverage GPU computing throughput through batching. Supported by pipelines."
      ],
      "metadata": {
        "id": "EhIKB182PGoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "batch = positive_tweets + negative_tweets + neutral_tweets\n",
        "random.shuffle(batch)\n",
        "\n",
        "# Create a pipeline for text classification, now in the GPU\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "\n",
        "results = classifier(batch)\n",
        "\n",
        "for input, output in zip(batch, results):\n",
        "  print(\"Input:\", input)\n",
        "  print(\"Output:\", output)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I7PlkwxPLQM",
        "outputId": "2f206790-7135-4b83-a648-714831d0c1e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I'm feeling really down today.\n",
            "Output: {'label': 'negative', 'score': 0.9055712223052979}\n",
            "\n",
            "Input: I lost my keys.\n",
            "Output: {'label': 'negative', 'score': 0.6730404496192932}\n",
            "\n",
            "Input: My computer is broken.\n",
            "Output: {'label': 'negative', 'score': 0.8859075307846069}\n",
            "\n",
            "Input: I'm so tired and stressed.\n",
            "Output: {'label': 'negative', 'score': 0.8532361388206482}\n",
            "\n",
            "Input: I'm going to the store.\n",
            "Output: {'label': 'neutral', 'score': 0.7757179737091064}\n",
            "\n",
            "Input: I just ate lunch.\n",
            "Output: {'label': 'neutral', 'score': 0.5950847268104553}\n",
            "\n",
            "Input: I hate traffic jams.\n",
            "Output: {'label': 'negative', 'score': 0.9240683913230896}\n",
            "\n",
            "Input: I'm listening to music.\n",
            "Output: {'label': 'neutral', 'score': 0.7014937400817871}\n",
            "\n",
            "Input: The weather is beautiful today.\n",
            "Output: {'label': 'positive', 'score': 0.9844391942024231}\n",
            "\n",
            "Input: I'm watching a movie.\n",
            "Output: {'label': 'neutral', 'score': 0.749224841594696}\n",
            "\n",
            "Input: It's raining outside.\n",
            "Output: {'label': 'neutral', 'score': 0.6317101120948792}\n",
            "\n",
            "Input: I'm so excited about my new job!\n",
            "Output: {'label': 'positive', 'score': 0.9876318573951721}\n",
            "\n",
            "Input: I love spending time with my family.\n",
            "Output: {'label': 'positive', 'score': 0.9865761995315552}\n",
            "\n",
            "Input: This is the best pizza I've ever had!\n",
            "Output: {'label': 'positive', 'score': 0.9853105545043945}\n",
            "\n",
            "Input: I just got a promotion!\n",
            "Output: {'label': 'positive', 'score': 0.9725865721702576}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other tasks\n",
        "\n",
        "There are many different tasks available through transformers. Here are some examples."
      ],
      "metadata": {
        "id": "YEPRR0DAQIoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition\n",
        "\n",
        "Word-level classification"
      ],
      "metadata": {
        "id": "YcBT35BWQYR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"elastic/distilbert-base-uncased-finetuned-conll03-english\"\n",
        "classifier = pipeline(\"ner\", model=model_name, device=\"cuda\")\n",
        "outputs = classifier([\n",
        "    \"The Data Science Institute is holding AI-related workshops for the university community.\",\n",
        "    \"Enrique and Carlos are hosting a workshop about HuggingFace on September\"\n",
        "    ])\n",
        "\n",
        "for output in outputs:\n",
        "  words = []\n",
        "  p_entity = None\n",
        "  for elem in output:\n",
        "    type_, entity = elem['entity'].split('-')\n",
        "    if type_ == \"B\":\n",
        "      print(p_entity, \" \".join(words))\n",
        "      words = []\n",
        "      p_entity = None\n",
        "    words.append(elem['word'])\n",
        "    p_entity = entity\n",
        "  print(p_entity, \"\".join(words))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9edZkdMCPSPZ",
        "outputId": "394eae9a-9791-41d7-9a9c-66ee82237213"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None \n",
            "ORG data science institute\n",
            "MISC ai\n",
            "MISC -\n",
            "MISC related\n",
            "\n",
            "None \n",
            "PER enrique\n",
            "PER carlos\n",
            "PER hugging\n",
            "PER ##face\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine translation\n",
        "Sequence to sequence generation"
      ],
      "metadata": {
        "id": "CuTHmGVZTkac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\", device=\"cuda\")\n",
        "\n",
        "text1 = \"Me gusta aprender sobre inteligencia artificial.\"\n",
        "text2 = \"Tengo ganas de comer una hamburguesa.\"\n",
        "\n",
        "text = [text1, text2]\n",
        "translated = translator(text)\n",
        "\n",
        "pprint(translated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7aukaBDSED-",
        "outputId": "cd7e7df8-5faa-4c7b-a4cc-fa03eaa6ec32"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': 'I like to learn about artificial intelligence.'},\n",
            " {'translation_text': 'I feel like eating a hamburger.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization\n",
        "Sequence to sequence generation"
      ],
      "metadata": {
        "id": "D0Kg8RA5WqyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=\"cuda\")\n",
        "\n",
        "ARTICLE = \"\"\" New York, NY – A fierce competition is brewing among tech giants as they vie to lead the burgeoning artificial intelligence (AI) market. Companies like Google, Microsoft, Meta, and Amazon are investing heavily in research and development, aiming to create AI systems that can revolutionize industries from healthcare to transportation.\n",
        "AI has the potential to transform society in unprecedented ways. From developing new medical treatments to automating complex tasks, AI applications are becoming increasingly sophisticated. However, the rapid advancement of AI also raises concerns about ethical implications, job displacement, and the potential for misuse.\n",
        "Google, a pioneer in AI research, has made significant strides in recent years. Its AI-powered search engine and Google Assistant have become ubiquitous in everyday life. Microsoft, not to be outdone, has invested heavily in AI through its acquisition of OpenAI, the company behind the popular language model GPT-3.\n",
        "Meta, formerly known as Facebook, is also making significant investments in AI. The company's AI research focuses on areas such as natural language processing, computer vision, and recommendation systems. Amazon, while primarily known for its e-commerce platform, has been leveraging AI to improve its customer experience and optimize its logistics operations.\n",
        "As the competition intensifies, the tech giants are facing increasing pressure to ensure that their AI systems are developed and deployed responsibly. Concerns about bias, privacy, and the potential for AI to be used for malicious purposes are becoming more prominent.\n",
        "The future of AI is uncertain, but one thing is clear: the race to dominate this emerging market is far from over. The tech giants that are able to successfully navigate the challenges and opportunities presented by AI will likely reap significant rewards for years to come.\n",
        "\"\"\"\n",
        "pprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTnhHtYZUZLY",
        "outputId": "a7dc8cc2-48a5-4c4c-b35a-a5990d79aed0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'A fierce competition is brewing among tech giants as they '\n",
            "                  'vie to lead the burgeoning artificial intelligence market. '\n",
            "                  'Google, Microsoft, Meta, and Amazon are investing heavily '\n",
            "                  'in research and development. Concerns about bias, privacy, '\n",
            "                  'and the potential for AI to be used for malicious purposes '\n",
            "                  'are becoming more prominent.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extractive question answering\n",
        "Span prediction"
      ],
      "metadata": {
        "id": "AIrgOAKVXI--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UY3TdKYkYSYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "oracle = pipeline('question-answering', model=model_name, device=\"cuda\")\n",
        "\n",
        "res = oracle({\n",
        "    'question': \"What is Neo's goal?\",\n",
        "    'context': \"\"\"The Matrix is a science fiction film set in a dystopian future where humanity is unknowingly trapped in a simulated reality created by intelligent machines.\n",
        "    The machines use humans as a power source and keep them unaware of their true situation.The story follows Neo, a computer hacker who is chosen to free humanity from this simulated world, known as the Matrix.\n",
        "    He is guided by a group of rebels who believe they can overthrow the machines and liberate humanity.\"\"\"\n",
        "})\n",
        "\n",
        "pprint(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rsL8BtxW32_",
        "outputId": "f98a9976-f9d3-461e-f17e-6cb00fbd6d54"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'free humanity from this simulated world',\n",
            " 'end': 346,\n",
            " 'score': 0.4039424657821655,\n",
            " 'start': 307}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Composing pipelines\n",
        "\n",
        "Given that pipelines are high level abstractions, we can compose them into more complex, multi-step pipelines fairly easily."
      ],
      "metadata": {
        "id": "4JaYfO4qa6_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "\n",
        "def combined_pipeline(text):\n",
        "  summary = summarizer(text, max_length=60, min_length=30, do_sample=False)\n",
        "  translation = translator(summary[0]['summary_text'])\n",
        "  return translation[0]['translation_text']\n",
        "\n",
        "combined_pipeline(ARTICLE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "I8pGxuOqYV1w",
        "outputId": "836263ab-f69a-4b94-9198-e21ae9ba9b60"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Las empresas como Google, Microsoft, Meta y Amazon están invirtiendo mucho en investigación y desarrollo. Las preocupaciones sobre el sesgo, la privacidad y el potencial para que la IA se utilice con fines maliciosos son cada vez más prominentes. Los gigantes tecnológicos que son capaces de navegar con éxito los desafíos y oportunidades presentados por la IA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUdxUTN_aj8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}